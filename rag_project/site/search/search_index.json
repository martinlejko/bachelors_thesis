{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RAG Project Documentation","text":"<p>This is the documentation for the Retrieval-Augmented Generation (RAG) project developed as part of a Bachelor's thesis.</p>"},{"location":"#project-overview","title":"Project Overview","text":"<p>This project implements a Retrieval-Augmented Generation (RAG) system that combines the power of large language models with a retrieval mechanism to provide accurate and contextually relevant responses to user queries. The system retrieves information from a knowledge base before generating responses, improving the accuracy and relevance of the answers.</p>"},{"location":"#project-structure","title":"Project Structure","text":"<ul> <li>proof_of_concept/: Contains the RAG pipeline implementation</li> <li>Core RAG functionality</li> <li>Document processing</li> <li>Vector database integration</li> <li> <p>Question answering chain</p> </li> <li> <p>testing/: Contains testing utilities and test cases</p> </li> <li>Test data definitions</li> <li>Evaluation metrics</li> <li>Test fixtures and utilities</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To run the RAG pipeline:</p> <pre><code>from proof_of_concept.proof_of_concept import load_data, process_data, create_vectorstore, setup_model, create_prompt_template, create_qa_chain\n\n# Example URLs\nurls = [\"https://example.com/document1\", \"https://example.com/document2\"]\n\n# Data pipeline\nraw_data = load_data(urls)\nprocessed_data = process_data(raw_data)\nvectorstore = create_vectorstore(processed_data)\n\n# Model and prompt setup\nmodel = setup_model()\nprompt = create_prompt_template()\n\n# Create QA chain\nqa_chain = create_qa_chain(vectorstore, model, prompt)\n\n# Ask a question\nquestion = \"What is RAG?\"\nresponse = qa_chain(question)\nprint(f\"Answer: {response['answer']}\")\n</code></pre>"},{"location":"api/rag_pipeline/","title":"RAG Pipeline","text":"<p>This section documents the core RAG (Retrieval-Augmented Generation) pipeline components.</p>"},{"location":"api/rag_pipeline/#core-module","title":"Core Module","text":"<p>Retrieval-Augmented Generation (RAG) Proof of Concept</p> <p>This module implements a basic RAG system using LangChain components and Ollama models. The system loads data from web URLs, processes it into chunks, embeds them into a vector store, and creates a question-answering chain that retrieves relevant context to answer user queries.</p>"},{"location":"api/rag_pipeline/#proof_of_concept.proof_of_concept.create_prompt_template","title":"<code>create_prompt_template()</code>","text":"<p>Create and return the RAG prompt template.</p> <p>Returns:</p> Type Description <code>ChatPromptTemplate</code> <p>ChatPromptTemplate configured for RAG question answering</p> Source code in <code>proof_of_concept/proof_of_concept.py</code> <pre><code>def create_prompt_template() -&gt; ChatPromptTemplate:\n    \"\"\"\n    Create and return the RAG prompt template.\n\n    Returns:\n        ChatPromptTemplate configured for RAG question answering\n    \"\"\"\n    template = \"\"\"\n    You are an assistant for question-answering tasks. Use the following pieces of retrieved \n    context to answer the question. If you don't know the answer, just say that you don't know. \n    Use three sentences maximum and keep the answer concise.\n    &lt;context&gt;\n    {context}\n    &lt;/context&gt;\n    Answer the following question:\n    {question}\n    \"\"\"\n    return ChatPromptTemplate.from_template(template)\n</code></pre>"},{"location":"api/rag_pipeline/#proof_of_concept.proof_of_concept.create_qa_chain","title":"<code>create_qa_chain(vectorstore, model, prompt)</code>","text":"<p>Create the question-answering chain that returns both answer and retrieved documents.</p> <p>Parameters:</p> Name Type Description Default <code>vectorstore</code> <code>Chroma</code> <p>Vector store with embedded documents</p> required <code>model</code> <code>ChatOllama</code> <p>Language model for answering questions</p> required <code>prompt</code> <code>ChatPromptTemplate</code> <p>Prompt template for the QA task</p> required <p>Returns:</p> Type Description <p>Function that takes a question and returns answer with retrieval context</p> Source code in <code>proof_of_concept/proof_of_concept.py</code> <pre><code>def create_qa_chain(vectorstore: Chroma, model: ChatOllama, prompt: ChatPromptTemplate):\n    \"\"\"\n    Create the question-answering chain that returns both answer and retrieved documents.\n\n    Args:\n        vectorstore: Vector store with embedded documents\n        model: Language model for answering questions\n        prompt: Prompt template for the QA task\n\n    Returns:\n        Function that takes a question and returns answer with retrieval context\n    \"\"\"\n    retriever = vectorstore.as_retriever()\n\n    # Create the standard QA chain\n    qa_chain = (\n        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n    )\n\n    def invoke_with_retrieval_context(question):\n        \"\"\"\n        Invoke the QA chain and return both the answer and retrieval context.\n\n        Args:\n            question: The question to answer\n\n        Returns:\n            Dictionary containing the answer and retrieval context\n        \"\"\"\n        retrieved_docs = retriever.invoke(question)\n        answer = qa_chain.invoke(question)\n\n        context_strings = [doc.page_content for doc in retrieved_docs]\n        return {\"answer\": answer, \"retrieval_context\": context_strings}\n\n    return invoke_with_retrieval_context\n</code></pre>"},{"location":"api/rag_pipeline/#proof_of_concept.proof_of_concept.create_vectorstore","title":"<code>create_vectorstore(documents, model_name='nomic-embed-text')</code>","text":"<p>Create and return a vector store from processed documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of Document objects to embed</p> required <code>model_name</code> <code>str</code> <p>Name of the embedding model to use</p> <code>'nomic-embed-text'</code> <p>Returns:</p> Type Description <code>Chroma</code> <p>Chroma vector store containing embeddings</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If vectorstore creation fails</p> Source code in <code>proof_of_concept/proof_of_concept.py</code> <pre><code>def create_vectorstore(documents: List[Document], model_name: str = \"nomic-embed-text\") -&gt; Chroma:\n    \"\"\"\n    Create and return a vector store from processed documents.\n\n    Args:\n        documents: List of Document objects to embed\n        model_name: Name of the embedding model to use\n\n    Returns:\n        Chroma vector store containing embeddings\n\n    Raises:\n        Exception: If vectorstore creation fails\n    \"\"\"\n    try:\n        embeddings = OllamaEmbeddings(model=model_name)\n        return Chroma.from_documents(documents=documents, embedding=embeddings)\n    except Exception as e:\n        print(f\"Error creating vectorstore: {e}\")\n        raise\n</code></pre>"},{"location":"api/rag_pipeline/#proof_of_concept.proof_of_concept.format_docs","title":"<code>format_docs(docs)</code>","text":"<p>Format retrieved documents into a single string.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Document objects to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>String containing all document contents joined by newlines</p> Source code in <code>proof_of_concept/proof_of_concept.py</code> <pre><code>def format_docs(docs: List[Document]) -&gt; str:\n    \"\"\"\n    Format retrieved documents into a single string.\n\n    Args:\n        docs: List of Document objects to format\n\n    Returns:\n        String containing all document contents joined by newlines\n    \"\"\"\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n</code></pre>"},{"location":"api/rag_pipeline/#proof_of_concept.proof_of_concept.load_data","title":"<code>load_data(urls)</code>","text":"<p>Load data from a list of web URLs into Document objects.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>List[str]</code> <p>List of URLs to fetch data from</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of Document objects containing the loaded content</p> Source code in <code>proof_of_concept/proof_of_concept.py</code> <pre><code>def load_data(urls: List[str]) -&gt; List[Document]:\n    \"\"\"\n    Load data from a list of web URLs into Document objects.\n\n    Args:\n        urls: List of URLs to fetch data from\n\n    Returns:\n        List of Document objects containing the loaded content\n    \"\"\"\n    loader = WebBaseLoader(urls)\n    return loader.load()\n</code></pre>"},{"location":"api/rag_pipeline/#proof_of_concept.proof_of_concept.main","title":"<code>main()</code>","text":"<p>Main function to demonstrate RAG pipeline functionality with example data.</p> Source code in <code>proof_of_concept/proof_of_concept.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to demonstrate RAG pipeline functionality with example data.\n    \"\"\"\n    # Example URLs\n    urls = [\n        \"https://d3s.mff.cuni.cz/teaching/nswi200/teams/\",\n        \"https://d3s.mff.cuni.cz/teaching/nprg035/\",\n        \"https://webik.ms.mff.cuni.cz/nswi142/\",\n        \"https://martinlejko.github.io/posts/hello-blog/\",\n    ]\n\n    # Data pipeline\n    raw_data = load_data(urls)\n    processed_data = process_data(raw_data)\n    vectorstore = create_vectorstore(processed_data)\n\n    # Model and prompt setup\n    model = setup_model()\n    prompt = create_prompt_template()\n\n    # Create QA chain\n    qa_chain = create_qa_chain(vectorstore, model, prompt)\n\n    # Example question\n    question = \"What is the amount of people in a team for operating systems course at MFF cuni?\"\n    response = qa_chain(question)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"QUESTION: {question}\")\n    print(\"-\" * 80)\n    print(f\"ANSWER: {response['answer']}\")\n    print(\"-\" * 80)\n    print(\"SOURCES:\")\n    for i, source in enumerate(response[\"retrieval_context\"], 1):\n        formatted_source = source.replace(\"\\n\", \"\")\n        print(\n            f\"\\n[Source {i}]:\\n{formatted_source[:300]}...\"\n            if len(formatted_source) &gt; 300\n            else f\"\\n[Source {i}]:\\n{formatted_source}\"\n        )\n    print(\"=\" * 80 + \"\\n\")\n</code></pre>"},{"location":"api/rag_pipeline/#proof_of_concept.proof_of_concept.process_data","title":"<code>process_data(documents, chunk_size=500, chunk_overlap=0)</code>","text":"<p>Split documents into chunks for processing.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of Document objects to process</p> required <code>chunk_size</code> <code>int</code> <p>Maximum size of each chunk in characters</p> <code>500</code> <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping characters between chunks</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of Document objects after splitting</p> Source code in <code>proof_of_concept/proof_of_concept.py</code> <pre><code>def process_data(documents: List[Document], chunk_size: int = 500, chunk_overlap: int = 0) -&gt; List[Document]:\n    \"\"\"\n    Split documents into chunks for processing.\n\n    Args:\n        documents: List of Document objects to process\n        chunk_size: Maximum size of each chunk in characters\n        chunk_overlap: Number of overlapping characters between chunks\n\n    Returns:\n        List of Document objects after splitting\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    return text_splitter.split_documents(documents)\n</code></pre>"},{"location":"api/rag_pipeline/#proof_of_concept.proof_of_concept.setup_model","title":"<code>setup_model(model_name='llama3.1:8b')</code>","text":"<p>Initialize and return the language model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the Ollama model to use</p> <code>'llama3.1:8b'</code> <p>Returns:</p> Type Description <code>ChatOllama</code> <p>Configured ChatOllama model instance</p> Source code in <code>proof_of_concept/proof_of_concept.py</code> <pre><code>def setup_model(model_name: str = \"llama3.1:8b\") -&gt; ChatOllama:\n    \"\"\"\n    Initialize and return the language model.\n\n    Args:\n        model_name: Name of the Ollama model to use\n\n    Returns:\n        Configured ChatOllama model instance\n    \"\"\"\n    return ChatOllama(model=model_name)\n</code></pre>"},{"location":"api/rag_pipeline/#main-functions","title":"Main Functions","text":"<p>The RAG pipeline consists of the following key components:</p> <ul> <li><code>load_data()</code>: Loads data from URLs</li> <li><code>process_data()</code>: Splits documents into chunks</li> <li><code>create_vectorstore()</code>: Creates vector embeddings</li> <li><code>setup_model()</code>: Initializes the language model</li> <li><code>create_prompt_template()</code>: Creates the RAG prompt</li> <li><code>create_qa_chain()</code>: Builds the question-answering chain </li> </ul>"},{"location":"api/testing/","title":"Testing Framework","text":"<p>This section documents the testing framework used to evaluate the RAG system's performance.</p>"},{"location":"api/testing/#test-configuration","title":"Test Configuration","text":"<p>Pytest configuration and shared fixtures</p>"},{"location":"api/testing/#proof_of_concept.testing.conftest.evaluation_metrics","title":"<code>evaluation_metrics()</code>","text":"<p>Define evaluation metrics used across tests.</p> Source code in <code>proof_of_concept/testing/conftest.py</code> <pre><code>@pytest.fixture(scope=\"session\")\ndef evaluation_metrics():\n    \"\"\"Define evaluation metrics used across tests.\"\"\"\n\n    return [\n        GEval(\n            name=\"Answer Correctness\",\n            criteria=\"Determine if the actual output correctly answers the question based on the expected output.\",\n            evaluation_params=[\n                LLMTestCaseParams.INPUT,\n                LLMTestCaseParams.ACTUAL_OUTPUT,\n                LLMTestCaseParams.EXPECTED_OUTPUT,\n                LLMTestCaseParams.CONTEXT,\n            ],\n            threshold=0.7,\n        ),\n        GEval(\n            name=\"Answer Relevancy\",\n            criteria=\"Evaluate if the response is directly relevant to the question asked.\",\n            evaluation_params=[\n                LLMTestCaseParams.INPUT,\n                LLMTestCaseParams.ACTUAL_OUTPUT,\n            ],\n            threshold=0.7,\n        ),\n        GEval(\n            name=\"Conciseness\",\n            criteria=\"Check if the response is within three sentences and provides a clear answer.\",\n            evaluation_params=[\n                LLMTestCaseParams.ACTUAL_OUTPUT,\n            ],\n            threshold=0.8,\n        ),\n        GEval(\n            name=\"Context Relevancy\",\n            criteria=\"Evaluate if the retrieved context contains information relevant to answering the query. The context should contain facts or information directly related to what the question is asking about.\",\n            evaluation_params=[\n                LLMTestCaseParams.INPUT,\n                LLMTestCaseParams.RETRIEVAL_CONTEXT,\n            ],\n            threshold=0.7,\n        ),\n    ]\n</code></pre>"},{"location":"api/testing/#proof_of_concept.testing.conftest.qa_pipeline","title":"<code>qa_pipeline()</code>","text":"<p>Initialize RAG pipeline once for all tests.</p> Source code in <code>proof_of_concept/testing/conftest.py</code> <pre><code>@pytest.fixture(scope=\"session\")\ndef qa_pipeline():\n    \"\"\"Initialize RAG pipeline once for all tests.\"\"\"\n    urls = [\n        \"https://d3s.mff.cuni.cz/teaching/nswi200/teams/\",\n        \"https://d3s.mff.cuni.cz/teaching/nprg035/\",\n    ]\n\n    raw_data = load_data(urls)\n    processed_data = process_data(raw_data)\n    vectorstore = create_vectorstore(processed_data)\n    model = setup_model()\n    prompt = create_prompt_template()\n    return create_qa_chain(vectorstore, model, prompt)\n</code></pre>"},{"location":"api/testing/#test-cases-data","title":"Test Cases Data","text":"<p>Test case definitions and expected results for RAG system evaluation.</p> <p>This module defines test cases used for evaluating the RAG system's performance. Each test case contains: - question: The query to be answered by the RAG system - expected_output: The expected answer from the system - context: Sample context data that contains the information needed to answer correctly</p>"},{"location":"api/testing/#rag-system-tests","title":"RAG System Tests","text":"<p>RAG system evaluation tests.</p> <p>This module contains pytest tests that evaluate the RAG pipeline's performance using test cases and metrics defined in the project. The tests use DeepEval  for running evaluations and generating test reports.</p>"},{"location":"api/testing/#proof_of_concept.testing.test_rag.test_all_dataset","title":"<code>test_all_dataset(qa_pipeline, evaluation_metrics)</code>","text":"<p>Test the RAG pipeline using EvaluationDatasetFactory on all test cases.</p> <p>This test: 1. Creates a test dataset from the defined test cases 2. Evaluates the RAG pipeline using the configured metrics 3. Saves test results and generates a report</p> <p>Parameters:</p> Name Type Description Default <code>qa_pipeline</code> <p>Fixture providing the configured RAG pipeline</p> required <code>evaluation_metrics</code> <p>Fixture providing the evaluation metrics</p> required Source code in <code>proof_of_concept/testing/test_rag.py</code> <pre><code>def test_all_dataset(qa_pipeline, evaluation_metrics):\n    \"\"\"\n    Test the RAG pipeline using EvaluationDatasetFactory on all test cases.\n\n    This test:\n    1. Creates a test dataset from the defined test cases\n    2. Evaluates the RAG pipeline using the configured metrics\n    3. Saves test results and generates a report\n\n    Args:\n        qa_pipeline: Fixture providing the configured RAG pipeline\n        evaluation_metrics: Fixture providing the evaluation metrics\n    \"\"\"\n    dataset = EvaluationDatasetFactory.create_from_dict_with_invocation(TEST_CASES, qa_pipeline)\n\n    result = evaluate(\n        test_cases=dataset.test_cases,\n        metrics=evaluation_metrics,\n    )\n\n    save_test_results(result, \"proof_of_concept\")\n    report_from_latest_json()\n</code></pre>"},{"location":"api/testing/#test-execution","title":"Test Execution","text":"<p>The tests can be run using pytest:</p> <pre><code>pytest proof_of_concept/testing/\n</code></pre> <p>The test results are saved and can be viewed in the generated reports. </p>"}]}